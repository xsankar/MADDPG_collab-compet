{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "Implement DDPG for the Reacher environment using the Unity ML-Agents toolkit \n",
    "\n",
    "The program has 3 parts :\n",
    "- Part 1 Defines the classes, initiates the environment and so forth. It sets up all the scaffolding needed\n",
    "- Part 2 Explore and Learn - it performs the DDPG Reinforcement Learning. It also saves the best model\n",
    "- Part 3 Run saved model\n",
    "- I have captured portion of the runs in the files p2_cc_no_learning_02.mov and p2_cc_after_learning_02.mov. So one can run the mp4 file to see how the agent behaves.\n",
    "\n",
    "So one can either :\n",
    "- Run the cells in Part 1 and then Part 2 -> to train a model, explore hyperparameters and so forth (slow)\n",
    "- `Or` \n",
    "- Run cells in Part 1 and then Part 3 -> to run a stored model (fast)\n",
    "- `Or`\n",
    "- Watch the two videos viz. `p2_cc_no_learning_02.mov` and `p2_cc_after_learning_02.mov` (fastest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Definitions & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Install the required packages\n",
    "\n",
    "The required setup is detailed in the README.md\n",
    "\n",
    "I am running this on a MacBookPro 14,3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define imports\n",
    "\n",
    "python 3, numpy, matplotlib, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import copy\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants Definitions\n",
    "BUFFER_SIZE = int(1e5) # int(1e6) # int(1e5)  # replay buffer size  ?\n",
    "BATCH_SIZE = 64 # 128 # 64 # 256        # minibatch size for training\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 5e-4 # 1e-4 # 0.001 # 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 6e-4 # 3e-4  # 3e-3 # 0.001 # 3e-4        # learning rate of the critic 0.001\n",
    "WEIGHT_DECAY = 0.0001   # L2 weight decay\n",
    "# Number of neurons in the layers of the Actor & Critic Networks\n",
    "FC_UNITS_ACTOR = [128,128] # [64,128] # [32,16] # [400,300] # [128,128]\n",
    "FC_UNITS_CRITIC = [128,128] # [64,128] # [32,16] # [400,300] # [128,128]\n",
    "# Store models flag. Store during calibration runs and do not store during hyperparameter search\n",
    "# Used in Part 3 to run a stored model\n",
    "STORE_MODELS = False # True - Turn it on when you are ready to do the calibration training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unity environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python.\n",
    "\n",
    "`Note : The file_name might be different for different OS. As mentioned earlier, I am running OSX on a MAcBookPro`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name='Reacher.app') #'Reacher_20.app')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of the agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "- The cell below tests to make sure the environment is up and running by printing some information about the environment.\n",
    "- It also acquires the dimensions of the state and action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -6.30408478e+00 -1.00000000e+00\n",
      " -4.92529202e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -5.33014059e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unity environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python.\n",
    "\n",
    "`Note : The file_name might be different for different OS. As mentioned earlier, I am running OSX on a MAcBookPro`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Define classes and setup\n",
    "\n",
    "The device declaration enables the program leverage GPUs if they are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device = {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Learning Algorithm\n",
    "\n",
    "We are using the DDPG as explained in the report.\n",
    "<img src=\"RL_Systems_Flow.png\" style=\"float:left\">\n",
    "\n",
    "The major components of the algorithm are:\n",
    "1. `Actor` implemented as a Deep Neural Network whih consists of fully connected layers.\n",
    "2. `Critic` implemented as a Deep Neural Network which consists of fully connected networks\n",
    "3. `Experience replay buffer` - in order to train the network we take actions and then store the results in the replay buffer. The replay buffer is a circular buffer and it has methods to sample a random batch\n",
    "3. `The Agent` brings all of the above together. It interacts with the environment by taking actions based on a policy, collects rewards and the observation feedback, then stores the experience in the replay buffer and also initiates a learning step on the actor and critic networks. The accompanying Report.pdf has more details on the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "# Finally used Xavier Initialization, which is very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size=state_size, action_size=action_size, seed=42, fc_units=FC_UNITS_ACTOR):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state, defaults to the global state size from the env\n",
    "            action_size (int): Dimension of each action, defaults to the global action size from the env\n",
    "            seed (int): Random seed\n",
    "            fc_units (list(int)): Number of nodes in the hidden layers as a list\n",
    "            ** Hard coded as a 3 layer network \n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            #nn.BatchNorm1d(state_size),\n",
    "            nn.Linear(state_size,fc_units[0]),\n",
    "            nn.ReLU(),\n",
    "            #nn.BatchNorm1d(fc_units[0]),\n",
    "            nn.Linear(fc_units[0],fc_units[1]),\n",
    "            nn.ReLU(),\n",
    "            #nn.BatchNorm1d(fc_units[1]),\n",
    "            nn.Linear(fc_units[1],action_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.model.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self,m):\n",
    "        if (type(m) == nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            #nn.init.xavier_normal_(m.weight)\n",
    "            # nn.init.kaiming_normal_(m.weight)\n",
    "            m.bias.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        return self.model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size = state_size, action_size = action_size, seed=42, fc_units=FC_UNITS_CRITIC):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state, defaults to the global state size from the env\n",
    "            action_size (int): Dimension of each action, defaults to the global action size from the env\n",
    "            seed (int): Random seed\n",
    "            fc_units (list(int)): Number of nodes in the hidden layers as a list\n",
    "            ** Hard coded as a 3 layer network \n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.hc_1 = nn.Sequential(\n",
    "            nn.Linear(state_size,fc_units[0]),\n",
    "            nn.ReLU(), # leaky relu ?\n",
    "            nn.BatchNorm1d(fc_units[0])\n",
    "        )\n",
    "        self.hc_2 = nn.Sequential(\n",
    "            nn.Linear(fc_units[0]+action_size,fc_units[1]),\n",
    "            nn.ReLU(), # leaky relu ?\n",
    "            nn.Linear(fc_units[1],1)\n",
    "        )\n",
    "        # Initialize the layers\n",
    "        self.hc_1.apply(self.init_weights)\n",
    "        self.hc_2.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = self.hc_1(state)\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = self.hc_2(x)\n",
    "        return (x)\n",
    "    \n",
    "    def init_weights(self,layer):\n",
    "        if (type(layer) == nn.Linear):\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            layer.bias.data.fill_(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed=42):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=False): #True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6. Instantiate an agent\n",
    "\n",
    "The state space and the action space dimensions come from the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=33, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=4, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n",
      "Critic(\n",
      "  (hc_1): Sequential(\n",
      "    (0): Linear(in_features=33, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (hc_2): Sequential(\n",
      "    (0): Linear(in_features=132, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=42)\n",
    "print(agent.actor_local)\n",
    "print(agent.critic_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Learn & Train\n",
    "-----\n",
    "#### _**Note : If you want to run a stored model, skip Part 2 and run the cells in Part 3 below after the run logs & test area**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. DDPG Algorithm\n",
    "\n",
    "Define the DDPG Algorithm. Once we have defined the foundations (network, buffer, actor, critic, agent and so forth), the DDPG is relatively easy. It has a few responsibilities:\n",
    "1. Orchastrate the episodes calling the appropriate methods\n",
    "2. Display a running commentry of the scores and episode count\n",
    "3. Check the success criterion for solving the environment i.e. if running average is > 30 and print the episode count\n",
    "4. Store the model with the maximum score\n",
    "5. Keep track of the scores for analytics at the end of the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=2000, max_t=700):\n",
    "    scores_window = deque(maxlen=100)\n",
    "    scores = []\n",
    "    score  = 0\n",
    "    max_score = -np.Inf\n",
    "    has_seen_30 = False\n",
    "    actions = np.zeros([num_agents, action_size])\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        # scores = np.zeros(num_agents) \n",
    "        state = states[0]\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        max_steps = 0\n",
    "        for t in range(max_t):\n",
    "            # print(state)\n",
    "            action = agent.act(state)\n",
    "            # next_state, reward, done, _ = env.step(action)\n",
    "            actions[0] = action # change for multi agent\n",
    "            # print(actions)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            # scores += env_info.rewards                       # update the score (for each agent)\n",
    "            state = states[0]\n",
    "            reward = rewards[0]\n",
    "            next_state = next_states[0]\n",
    "            done = dones[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "            max_steps += 1\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}\\tMax_steps : {}'.\\\n",
    "              format(i_episode, np.mean(scores_window), score, max_steps), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))  \n",
    "        if (np.mean(scores_window) >= 30.0) and (not has_seen_30):\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:5.2f}'.\\\n",
    "                  format(i_episode-100, np.mean(scores_window)))\n",
    "            # torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            has_seen_30 = True\n",
    "            # break\n",
    "            # To see how far it can go\n",
    "        # Store the best model if desired\n",
    "        if STORE_MODELS:\n",
    "            if np.mean(scores_window) > max_score:\n",
    "                max_score = np.mean(scores_window)\n",
    "                torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "                torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "                # print(' .. Storing with score {}'.format(max_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. The actual training Run\n",
    "\n",
    "1. Run the DDPG\n",
    "2. Calculate and display end-of-run analytics viz. descriptive statistics and a plot of the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.95\n",
      "Episode 200\tAverage Score: 6.55\n",
      "Episode 300\tAverage Score: 27.73\n",
      "Episode 315\tAverage Score: 30.06\tScore: 25.23\tMax_steps : 1000\n",
      "Environment solved in 215 episodes!\tAverage Score: 30.06\n",
      "Episode 400\tAverage Score: 31.26\n",
      "Episode 500\tAverage Score: 32.39\n",
      "Episode 600\tAverage Score: 31.73\n",
      "Episode 700\tAverage Score: 32.58\n",
      "Episode 800\tAverage Score: 30.36\n",
      "Episode 900\tAverage Score: 32.20\n",
      "Episode 1000\tAverage Score: 34.17\n",
      "Episode 1100\tAverage Score: 32.92\n",
      "Episode 1200\tAverage Score: 35.36\n",
      "Episode 1300\tAverage Score: 31.67\n",
      "Episode 1400\tAverage Score: 33.16\n",
      "Episode 1500\tAverage Score: 34.33\n",
      "Elapsed : 4:25:37.281611\n",
      "2018-10-19 03:56:31.655755\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=33, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=4, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n",
      "Max Score 39.689999 at 1000\n",
      "Percentile [25,50,75] : [24.98499944 32.81499927 36.83249918]\n",
      "Variance : 131.885\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "scores = ddpg(n_episodes=1500,max_t=1500) # 2000,1500 ; quick test 100,500\n",
    "# The env ends at 1000 steps. Tried max_t > 1K. Didn't see any complex adaptive temporal behavior\n",
    "env.close() # Close the environment\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "print(agent.actor_local)\n",
    "# print(agent.critic_local)\n",
    "print('Max Score {:2f} at {}'.format(np.max(scores), np.argmax(scores)))\n",
    "print('Percentile [25,50,75] : {}'.format(np.percentile(scores,[25,50,75])))\n",
    "print('Variance : {:.3f}'.format(np.var(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Run logs & Notes\n",
    "\n",
    "1. A place to keep the statistics and qualitative observations\n",
    "2. It is easier to keep notes as soon as a run is done\n",
    "3. Also a place to keep future explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Logs\n",
    "[10/17/18] 1st try - after housekeeping errors \n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size 100000 ?\n",
    "BATCH_SIZE = 256        # minibatch size for training\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor 0.001\n",
    "LR_CRITIC = 3e-4        # learning rate of the critic 0.001\n",
    "WEIGHT_DECAY = 0.0001   # L2 weight decay\n",
    "# Number of neurons in the layers of the Actor & Critic Networks\n",
    "FC_UNITS_ACTOR = [128,128]\n",
    "FC_UNITS_CRITIC = [128,128]\n",
    "# Store models flag. Store during calibration runs and do not store during hyperparameter search\n",
    "STORE_MODELS = False\n",
    "Episode 100\tAverage Score: 3.86\n",
    "Episode 200\tAverage Score: 34.90\n",
    "Episode 275\tAverage Score: 94.06\tScore: 166.35 error in score, was never zeroed ! \n",
    "Steps 700\n",
    "\n",
    "[After correction]\n",
    "Steps = 1500, effectively 1000\n",
    "LR_ACTOR=0.001 \n",
    "Episode 84\tAverage Score: 0.23\tScore: 1.48\tMax_steps : 1001\n",
    "back to small learning, buffer = 100000\n",
    "batch=64\n",
    "Episode 100\tAverage Score: 0.44\n",
    "Episode 200\tAverage Score: 0.68\n",
    "Episode 292\tAverage Score: 1.08\tScore: 1.41\tMax_steps : 1000\n",
    "\n",
    "[Next-parameters from the paper 11:26 PM]\n",
    "# Constants Definitions\n",
    "BUFFER_SIZE = int(1e6) # int(1e5)  # replay buffer size  ?\n",
    "BATCH_SIZE = 64 # 256        # minibatch size for training\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4 # 0.001 # 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3 # 0.001 # 3e-4        # learning rate of the critic 0.001\n",
    "WEIGHT_DECAY = 0.0001   # L2 weight decay\n",
    "# Number of neurons in the layers of the Actor & Critic Networks\n",
    "FC_UNITS_ACTOR = [400,300] # [128,128]\n",
    "FC_UNITS_CRITIC = [400,300] # [128,128]\n",
    "# Store models flag. Store during calibration runs and do not store during hyperparameter search\n",
    "STORE_MODELS = False\n",
    "\n",
    "DDPG slow to learn ? \n",
    "\n",
    "Ran from 11 PM to 7 AM. Didn't learn anything ! 2000/500\n",
    "\n",
    "Goig back to old Episode 100\tAverage Score: 0.01\n",
    "Episode 200\tAverage Score: 0.01\n",
    "Episode 300\tAverage Score: 0.01\n",
    "Episode 400\tAverage Score: 0.01\n",
    "Episode 500\tAverage Score: 0.01\n",
    "Episode 600\tAverage Score: 0.02\n",
    "Episode 700\tAverage Score: 0.01\n",
    "Episode 739\tAverage Score: 0.01\tScore: 0.00\tMax_steps : 1000\n",
    "\n",
    "Episode 100\tAverage Score: 0.02\n",
    "Elapsed : 0:18:22.147297\n",
    "2018-10-18 09:19:18.366035\n",
    "\n",
    "Learned nothing\n",
    "Fit network and Learning rate to domain, get some life ie learning before optimizing, correctness\n",
    "try with small runs 100/500\n",
    "\n",
    "[10/18/18 9:21 AM] smaller LR ? lr 0.001, buffer = 1e5 No learning. Random, small values \n",
    "\n",
    "11:14 Go small [32,16] lr 0.001\n",
    "less time, there is hope\n",
    "Episode 100\tAverage Score: 0.49\n",
    "Elapsed : 0:04:44.577345\n",
    "\n",
    "** Run for 1000 episodes \n",
    "Episode 100\tAverage Score: 0.49\n",
    "Episode 200\tAverage Score: 0.40\n",
    "Episode 300\tAverage Score: 0.37\n",
    "Episode 394\tAverage Score: 0.39\tScore: 0.14\tMax_steps : 500\n",
    "\n",
    "** No improvement in learning - lr = 1e5 ?\n",
    "Nope\n",
    "Episode 100\tAverage Score: 0.33\n",
    "Episode 200\tAverage Score: 0.21\n",
    "Episode 280\tAverage Score: 0.18\tScore: 0.32\tMax_steps : 500\n",
    "\n",
    "** [64,64], lr 1e-4/3e-4\n",
    "Looking for progressive learning\n",
    "Increasing very slowly - still not enough; avg not even > 1 till 300. but starts picking up by 500. just above 1\n",
    "follows the ball, looks somewhat decent\n",
    "Episode 100\tAverage Score: 0.28\n",
    "Episode 200\tAverage Score: 0.60\n",
    "Episode 300\tAverage Score: 0.68\n",
    "Episode 400\tAverage Score: 0.82\n",
    "Episode 500\tAverage Score: 1.64\n",
    "Elapsed : 0:29:54.799176\n",
    "2018-10-18 12:51:39.506876\n",
    "** lr 1e-3,3e-3,xavier_normal\n",
    "Does this have the fidelity to learn, even slow ?\n",
    "Episode 100\tAverage Score: 0.31\n",
    "Episode 200\tAverage Score: 0.35\n",
    "Episode 300\tAverage Score: 0.48\n",
    "Episode 380\tAverage Score: 0.50\tScore: 0.79\tMax_steps : 500\n",
    "== Not making enough progress. Go back to 1e-4 et al\n",
    "** xavier normal to uniform\n",
    "no change \n",
    "Episode 100\tAverage Score: 0.28\n",
    "Episode 200\tAverage Score: 0.60\n",
    "Episode 300\tAverage Score: 0.68\n",
    "Episode 400\tAverage Score: 0.82\n",
    "Episode 500\tAverage Score: 1.64\n",
    "Elapsed : 0:26:48.989494\n",
    "2018-10-18 15:19:03.216973\n",
    "** repplay buffer 1e6 from 1e5 - any difference ?\n",
    "** batch size 128 ?\n",
    "Slightly delayed learning with RPB of 1e6\n",
    "Episode 100\tAverage Score: 0.26\n",
    "Episode 200\tAverage Score: 0.49\n",
    "Episode 300\tAverage Score: 0.61\n",
    "Episode 400\tAverage Score: 0.81\n",
    "Episode 500\tAverage Score: 1.00\n",
    "Elapsed : 0:43:10.432368\n",
    "2018-10-18 16:05:04.091380\n",
    "** [64,128] ? increase network size ? 1e5/BatchSize=64/\n",
    "Initially less = expected as larger nn\n",
    "Does it get better ? ie fidelity/less bias\n",
    "SHifting to aws. Too slow on mac - even this is 2.9 Core I7 w/4 cores. It drains the computer, 87% user, 692.8% python\n",
    "Gets better at 500. May be keep this and see if it can solve\n",
    "Episode 100\tAverage Score: 0.12\n",
    "Episode 200\tAverage Score: 0.04\n",
    "Episode 300\tAverage Score: 0.08\n",
    "Episode 400\tAverage Score: 0.43\n",
    "Episode 500\tAverage Score: 1.96\n",
    "Elapsed : 0:44:14.258167\n",
    "2018-10-18 16:51:13.878287\n",
    "\n",
    "Takes more time, but score is increasing ! 350 episodes just to get off the ground vs 100 for the 64 X 64\n",
    "** [128/128 ?] Will this give more fidelity ? Not moving fast, but slightly better. May be good with 20 agents\n",
    "Very depressing to see the learning\n",
    "\n",
    "** [64 X 128] aws-slightly different result\n",
    "Episode 100\tAverage Score: 0.16\tScore: 0.06\tMax_steps : 500\n",
    "Episode 200\tAverage Score: 0.02\tScore: 0.10\tMax_steps : 500\n",
    "Episode 300\tAverage Score: 0.05\tScore: 0.10\tMax_steps : 500\n",
    "Episode 400\tAverage Score: 0.15\tScore: 0.33\tMax_steps : 500\n",
    "Episode 500\tAverage Score: 0.43\tScore: 0.00\tMax_steps : 500\n",
    "Elapsed : 0:46:38.138455\n",
    "2018-10-19 00:25:58.326463\n",
    "\n",
    "** [128 X 128, lr=1e-5]\n",
    "Episode 100\tAverage Score: 0.14\n",
    "Episode 200\tAverage Score: 0.24\n",
    "Episode 300\tAverage Score: 0.47\n",
    "Episode 400\tAverage Score: 0.82\n",
    "Episode 500\tAverage Score: 1.15\n",
    "Elapsed : 0:37:13.352981\n",
    "2018-10-18 17:32:17.179289\n",
    "== Slightly better. Higher fidelity ?\n",
    "\n",
    "** [128 X 128, lr=1e-3/3e3 ?] aws w/ GPU ! 10/18/18 17:11\n",
    "1e-3 doesn't look that promising.\n",
    "Stopping aws GPU, back to lr-1e-5 and for 2000/1500. Long run with GPU at aws\n",
    "mac - still nowhere near I want it to be scoes in 0.xx for < 500 episodes. Need more because of single agent ? Be patient ?\n",
    "\n",
    "10/18/18 6:12 PM ** Start multi agent while running single agent in aws. See if it can solve it\n",
    "\n",
    "Found minor error states vs state - does it change anything ?\n",
    "\n",
    "** after error correction: No difference !\n",
    "Episode 100\tAverage Score: 0.14\n",
    "Episode 200\tAverage Score: 0.24\n",
    "Episode 300\tAverage Score: 0.47\n",
    "Episode 400\tAverage Score: 0.82\n",
    "Episode 500\tAverage Score: 1.15\n",
    "Elapsed : 0:34:15.877401\n",
    "2018-10-18 19:17:49.175497\n",
    "\n",
    "*** batchnorm error - works in Critic - and makes lot of difference\n",
    "w/batchnorm 128/128 lr=1e-4,3e-4\n",
    "with just one batchnorm, got scores > 10 at 450 episode !\n",
    "\n",
    "Episode 100\tAverage Score: 0.35\n",
    "Episode 200\tAverage Score: 0.51\n",
    "Episode 300\tAverage Score: 0.83\n",
    "Episode 400\tAverage Score: 2.36\n",
    "Episode 500\tAverage Score: 7.59\n",
    "Elapsed : 0:48:33.084493\n",
    "2018-10-18 21:07:03.470019\n",
    "\n",
    "works well, with a max of 19 in 500 episodes !\n",
    "\n",
    "*** trying 1500 to see if it solves it\n",
    "Episode 100\tAverage Score: 0.35\n",
    "Episode 200\tAverage Score: 0.51\n",
    "Episode 300\tAverage Score: 0.83\n",
    "Episode 400\tAverage Score: 2.36\n",
    "Episode 500\tAverage Score: 7.59\n",
    "Episode 600\tAverage Score: 17.18\n",
    "Episode 643\tAverage Score: 18.77\tScore: 19.60\tMax_steps : 500\n",
    "\n",
    "with 500, max score ~20. To get 30, need to increase to 1000\n",
    "Printing max steps as a metric helped here to see the internals\n",
    "\n",
    "[10/18/18 11:25 PM] Stopping and restarting from beginning and storing the model as well\n",
    "*** 1500/1500\n",
    "Bumping the lr to 5e-4;7e-4\n",
    "Learning fast ! Episode 5\tAverage Score: 1.13\tScore: 0.97\tMax_steps : 1000\n",
    "Yep, it worked !\n",
    "\n",
    "Can it keep it ? The bump is for steps from 500 to 1000. Not the ability ! Yet !\n",
    "Episode 100\tAverage Score: 0.95\n",
    "Episode 200\tAverage Score: 6.55\n",
    "Episode 300\tAverage Score: 27.73\n",
    "Episode 315\tAverage Score: 30.06\tScore: 25.23\tMax_steps : 1000\n",
    "Environment solved in 215 episodes!\tAverage Score: 30.06\n",
    "Episode 400\tAverage Score: 31.26\n",
    "Episode 500\tAverage Score: 32.39\n",
    "Episode 600\tAverage Score: 31.73\n",
    "Episode 700\tAverage Score: 32.58\n",
    "Episode 800\tAverage Score: 30.36\n",
    "Episode 900\tAverage Score: 32.20\n",
    "Episode 1000\tAverage Score: 34.17\n",
    "Episode 1100\tAverage Score: 32.92\n",
    "Episode 1200\tAverage Score: 35.36\n",
    "Episode 1300\tAverage Score: 31.67\n",
    "Episode 1400\tAverage Score: 33.16\n",
    "Episode 1500\tAverage Score: 34.33\n",
    "Elapsed : 4:25:37.281611\n",
    "2018-10-19 03:56:31.655755\n",
    "\n",
    "Questions:\n",
    "1. What is the score rate (as epochs increase) a function of ?\n",
    "  The actor and critic network plays a large part\n",
    "2. Does other init make a difference ? ** kaming_normal\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Run a stored Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Here we are saving and loading the state dict, because we have access to the code.\n",
    "\n",
    "Another way to save and load model, to be used by 2 distinct and separate entities is to :\n",
    "- `torch.save(model, filepath)`; \n",
    "- Then later, `model = torch.load(filepath)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from the saved file\n",
    "# The file has the parameters of the model that has the highest score during training\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :  1 Score : 37.95 Steps : 1000\n",
      "Episode :  2 Score : 35.11 Steps : 1000\n",
      "Episode :  3 Score : 25.45 Steps : 1000\n",
      "Episode :  4 Score : 35.53 Steps : 1000\n",
      "Episode :  5 Score : 35.05 Steps : 1000\n",
      "Episode :  6 Score : 35.32 Steps : 1000\n",
      "Episode :  7 Score : 25.24 Steps : 1000\n",
      "Episode :  8 Score : 34.45 Steps : 1000\n",
      "Episode :  9 Score : 32.97 Steps : 1000\n",
      "Episode : 10 Score : 37.68 Steps : 1000\n",
      "Episode : 11 Score : 35.46 Steps : 1000\n",
      "Episode : 12 Score : 33.26 Steps : 1000\n",
      "Episode : 13 Score : 35.10 Steps : 1000\n",
      "Episode : 14 Score : 32.94 Steps : 1000\n",
      "Episode : 15 Score : 34.01 Steps : 1000\n",
      "Episode : 16 Score : 33.61 Steps : 1000\n",
      "Episode : 17 Score : 21.89 Steps : 1000\n",
      "Episode : 18 Score : 34.26 Steps : 1000\n",
      "Episode : 19 Score : 30.70 Steps : 1000\n",
      "Episode : 20 Score : 38.00 Steps : 1000\n",
      "Episode : 21 Score : 37.35 Steps : 1000\n",
      "Episode : 22 Score : 26.13 Steps : 1000\n",
      "Episode : 23 Score : 36.40 Steps : 1000\n",
      "Episode : 24 Score : 36.99 Steps : 1000\n",
      "Episode : 25 Score : 35.78 Steps : 1000\n",
      "Episode : 26 Score : 37.70 Steps : 1000\n",
      "Episode : 27 Score : 36.21 Steps : 1000\n",
      "Episode : 28 Score : 27.82 Steps : 1000\n",
      "Episode : 29 Score : 36.11 Steps : 1000\n",
      "Episode : 30 Score : 36.21 Steps : 1000\n",
      "Episode : 31 Score : 38.68 Steps : 1000\n",
      "Episode : 32 Score : 38.17 Steps : 1000\n",
      "Episode : 33 Score : 24.71 Steps : 1000\n",
      "Episode : 34 Score : 33.96 Steps : 1000\n",
      "Episode : 35 Score : 30.94 Steps : 1000\n",
      "Episode : 36 Score : 34.78 Steps : 1000\n",
      "Episode : 37 Score : 35.66 Steps : 1000\n",
      "Episode : 38 Score : 35.54 Steps : 1000\n",
      "Episode : 39 Score : 35.61 Steps : 1000\n",
      "Episode : 40 Score : 26.41 Steps : 1000\n",
      "Episode : 41 Score : 39.25 Steps : 1000\n",
      "Episode : 42 Score : 38.06 Steps : 1000\n",
      "Episode : 43 Score : 31.07 Steps : 1000\n",
      "Episode : 44 Score : 22.22 Steps : 1000\n",
      "Episode : 45 Score : 34.33 Steps : 1000\n",
      "Episode : 46 Score : 35.71 Steps : 1000\n",
      "Episode : 47 Score : 35.39 Steps : 1000\n",
      "Episode : 48 Score : 22.22 Steps : 1000\n",
      "Episode : 49 Score : 35.23 Steps : 1000\n",
      "Episode : 50 Score : 31.06 Steps : 1000\n",
      "Mean of 50 episodes = 33.39359925359488\n",
      "Elapsed : 0:02:03.828875\n",
      "2018-10-19 19:17:58.502118\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "scores=[]\n",
    "for i in range(10): # 10 episodes\n",
    "    actions = np.zeros([num_agents, action_size])\n",
    "    env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "    state = env_info.vector_observations[0]            # get the current state\n",
    "    score = 0                                          # initialize the score\n",
    "    steps = 0                                          # Keep track of the number of steps\n",
    "    while True:\n",
    "        action = agent.act(state)                      # select an action\n",
    "        actions[0] = action                            # change for multi agent\n",
    "        env_info = env.step(actions)[brain_name]       # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0]   # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "        score += reward                                # update the score\n",
    "        state = next_state                             # roll over the state to next time step\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            break\n",
    "        else:\n",
    "            steps += 1\n",
    "    scores.append(score)\n",
    "    print(\"Episode : {:2d} Score : {:5.2f} Steps : {}\".format(i+1,score,steps))\n",
    "# Print stats at the end the run\n",
    "print('Mean of {} episodes = {}'.format(i+1,np.mean(scores)))\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
